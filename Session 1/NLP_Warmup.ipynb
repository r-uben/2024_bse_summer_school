{"cells":[{"cell_type":"markdown","id":"1589caf1","metadata":{"id":"1589caf1"},"source":["![bse_logo_textminingcourse](https://bse.eu/sites/default/files/styles/bgse_image_highlighted_image/public/carrousel/BSE_Summer_School.jpg?itok=rz2Sx5Ok)"]},{"cell_type":"markdown","id":"bdf5354d","metadata":{"id":"bdf5354d"},"source":["# Harnessing Language Models: Your Path to NLP Expert - Warmup\n","\n","# Introduction to Natural Language Processing (NLP)"]},{"cell_type":"markdown","id":"b8afc81b","metadata":{"id":"b8afc81b"},"source":["This notebook introduces you to pre-processing text. This is a very important part of text analysis when using the bag-of-words model and we will spend some time doing things by \"hand\" that we later use packages for. We are also using a corpus of just a few sentences where each sentence is treated as a document to illustrate the idea of the document term matrix that is central to the bag of words model.\n","\n","We will do this to understand why we are pre-processing in a particular way and how pre-processing can be used to explore the corpus.\n","\n","We will go through all steps:\n","- **Tokenization**\n","\n","- **Normalization**\n","  - Lemmatizing\n","  - Removing punctuation\n","  - Unifying text (e.g., converting to lower case)\n","  - Removing stopwords\n","  - Stemming\n","\n","- **Counting (N-grams) -> Document-Term Matrix (Vectorization)**\n","\n","This is not the order you typically see on webpages talking about pre-processing but I think it is the actual order in which you approach things chronologically. Lemmatizing is the really odd one in the order. It is also the most complex and the least well-packaged. So worth spending a little time on.\n","\n","We will ignore N-grams today when making counts of unigrams just to keep things as simple as possible.\n","\n","Important: later parts of the course will ONLY share the tokenization step in this pipeline and the tokenizer used will be very different. This is because modern NLP does not rely on the bag-of-words model. However, bag-of-words is still often used in actual NLP applications and so it is useful to know."]},{"cell_type":"markdown","id":"zyrDJbxu29Cb","metadata":{"id":"zyrDJbxu29Cb"},"source":["Before we start we need to install a few things."]},{"cell_type":"code","execution_count":1,"id":"3ee82b62","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ee82b62","outputId":"386968b5-378e-47b5-a189-53ff4a2c74bd","executionInfo":{"status":"ok","timestamp":1719835689981,"user_tz":-120,"elapsed":60714,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!pip install pandas\n","!pip install nltk\n","!pip install spacy\n","!python -m spacy download en_core_web_sm\n"]},{"cell_type":"code","execution_count":2,"id":"830f9270","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"830f9270","outputId":"13644fe4-da5b-4eec-bfa3-3a927c2f45bb","executionInfo":{"status":"ok","timestamp":1719835694499,"user_tz":-120,"elapsed":4530,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n"]},{"cell_type":"markdown","id":"88618dba","metadata":{"id":"88618dba"},"source":["# Tokenizing\n","\n","Tokenization is the process of breaking down a string of text into smaller pieces, called tokens. In the context of text mining and natural language processing (NLP), tokenization refers to the process of splitting text into words, phrases, symbols, or other meaningful elements, known as tokens."]},{"cell_type":"code","execution_count":3,"id":"59308326","metadata":{"id":"59308326","executionInfo":{"status":"ok","timestamp":1719835704772,"user_tz":-120,"elapsed":10277,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[],"source":["import pandas as pd\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","import spacy\n","sp = spacy.load('en_core_web_sm')\n","\n","import re\n","\n","from nltk.corpus import stopwords\n"]},{"cell_type":"code","execution_count":4,"id":"26136d83","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26136d83","outputId":"63148553-1665-4261-dbfd-ca1c4c068884","executionInfo":{"status":"ok","timestamp":1719835704773,"user_tz":-120,"elapsed":20,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'president', 'of', 'the', 'United', 'States', '(', 'US', ')', 'is', 'Joe', 'Biden', '.']\n","['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country', '.']\n","['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden', '?']\n","['A', 'really', 'well-known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh', '.']\n","['The', 'leaves', 'were', 'left', 'untouched', ',', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies', '.']\n"]}],"source":["# Define some example sentences\n","sentence1 = \"The president of the United States (US) is Joe Biden.\"\n","sentence2 = \"Joe Biden the president wants us in a united country.\"\n","sentence3 = \"Did a known artist paint portraits of Joe Biden?\"\n","sentence4 = \"A really well-known portrait artist is Vincent van Gogh.\"\n","sentence5 = \"The leaves were left untouched, and the bats were unable to bat away the flies.\"\n","\n","# Tokenize the sentences\n","tokens1 = nltk.word_tokenize(sentence1)\n","tokens2 = nltk.word_tokenize(sentence2)\n","tokens3 = nltk.word_tokenize(sentence3)\n","tokens4 = nltk.word_tokenize(sentence4)\n","tokens5 = nltk.word_tokenize(sentence5)\n","\n","# Tag the tokens with their part of speech\n","print(tokens1)\n","print(tokens2)\n","print(tokens3)\n","print(tokens4)\n","print(tokens5)"]},{"cell_type":"markdown","id":"635d131e","metadata":{"id":"635d131e"},"source":["## Lemmatizer\n","\n","Lemmatizer actually uses grammar - we can tell the lemmatizer through the position of the word where the word stands. But for this we need to do leave the sentence unprocessed.\n","\n","Alternatively, we can pass sentences and the lemmatizer figures it out by itself. A first example of what the lemmatizer can figure out by itself."]},{"cell_type":"code","execution_count":5,"id":"1055ce9c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1055ce9c","outputId":"2971c27a-26ac-429c-fead-01d78419aa19","executionInfo":{"status":"ok","timestamp":1719835707934,"user_tz":-120,"elapsed":3179,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["rocks : rock\n","corpora : corpus\n"]}],"source":["print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n","print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"]},{"cell_type":"code","execution_count":6,"id":"a7fd1328","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7fd1328","outputId":"6c9fc9ed-80c8-4ac9-d41e-8e39e95cd26a","executionInfo":{"status":"ok","timestamp":1719835707934,"user_tz":-120,"elapsed":13,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'president', 'of', 'the', 'United', 'States', '(', 'US', ')', 'is', 'Joe', 'Biden', '.']\n","The president of the United States ( US ) is Joe Biden .\n","['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country', '.']\n","Joe Biden the president want u in a united country .\n","['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden', '?']\n","Did a known artist paint portrait of Joe Biden ?\n","['A', 'really', 'well-known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh', '.']\n","A really well-known portrait artist is Vincent van Gogh .\n","['The', 'leaves', 'were', 'left', 'untouched', ',', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies', '.']\n","The leaf were left untouched , and the bat were unable to bat away the fly .\n"]}],"source":["sentences=[tokens1,tokens2,tokens3,tokens4, tokens5]\n","\n","for sentence in sentences:\n","    print(sentence)\n","    lemmatized_sentence = []\n","    for word in sentence:\n","        lemmatized_sentence.append(lemmatizer.lemmatize(word))\n","    print(\" \".join(lemmatized_sentence))"]},{"cell_type":"markdown","id":"80b45889","metadata":{"id":"80b45889"},"source":["This has very little impact on the text. However, lemmatizing becomes much more powerful if use the function of words in a text. We will now switch to the package spacy quickly because it has an immediate implementation of positioning.\n","\n","Below, spacy is called as sp. We pass it the sentence and it generates an object saved under sp_text. Spacy is very powerful with lots of functions. We will only use the word positioning which is stored under word.pos_. For more read this here: https://spacy.io/usage/spacy-101"]},{"cell_type":"code","execution_count":7,"id":"beef1217","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"beef1217","outputId":"a5bd61d5-236c-4c23-86ef-0c4b47915cd1","executionInfo":{"status":"ok","timestamp":1719835708308,"user_tz":-120,"elapsed":386,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The president of the United States (US) is Joe Biden.\n","The DET\n","president NOUN\n","of ADP\n","the DET\n","United PROPN\n","States PROPN\n","( PUNCT\n","US PROPN\n",") PUNCT\n","is AUX\n","Joe PROPN\n","Biden PROPN\n",". PUNCT\n","the president of the United States ( US ) be Joe Biden .\n"," \n","Joe Biden the president wants us in a united country.\n","Joe PROPN\n","Biden PROPN\n","the DET\n","president PROPN\n","wants VERB\n","us PRON\n","in ADP\n","a DET\n","united PROPN\n","country NOUN\n",". PUNCT\n","Joe Biden the president want we in a united country .\n"," \n","Did a known artist paint portraits of Joe Biden?\n","Did VERB\n","a DET\n","known ADJ\n","artist NOUN\n","paint NOUN\n","portraits NOUN\n","of ADP\n","Joe PROPN\n","Biden PROPN\n","? PUNCT\n","do a known artist paint portrait of Joe Biden ?\n"," \n","A really well-known portrait artist is Vincent van Gogh.\n","A DET\n","really ADV\n","well ADV\n","- PUNCT\n","known VERB\n","portrait NOUN\n","artist NOUN\n","is AUX\n","Vincent PROPN\n","van PROPN\n","Gogh PROPN\n",". PUNCT\n","a really well - know portrait artist be Vincent van Gogh .\n"," \n","The leaves were left untouched, and the bats were unable to bat away the flies.\n","The DET\n","leaves NOUN\n","were AUX\n","left VERB\n","untouched ADJ\n",", PUNCT\n","and CCONJ\n","the DET\n","bats NOUN\n","were AUX\n","unable ADJ\n","to PART\n","bat VERB\n","away ADV\n","the DET\n","flies NOUN\n",". PUNCT\n","the leave be leave untouched , and the bat be unable to bat away the fly .\n"," \n"]}],"source":["sentences=[sentence1,sentence2,sentence3,sentence4, sentence5]\n","\n","for sentence in sentences:\n","    print(sentence)\n","    sp_text=sp(sentence)\n","    lemmatized_sentence = []\n","    for word in sp_text:\n","        print(word, word.pos_)\n","        lemmatized_sentence.append(word.lemma_)\n","    print(\" \".join(lemmatized_sentence))\n","    print(\" \")\n"]},{"cell_type":"markdown","id":"f5a92b7b","metadata":{"id":"f5a92b7b"},"source":["## Remove punctuation and unify, e.g. convert to lower case\n","\n","We now start back from the tokenized text as if we did not decide to do lemmatizing. Obviously, this is not what you want to do if lemmatizing is part of your pipeline."]},{"cell_type":"markdown","id":"66e3be17","metadata":{"id":"66e3be17"},"source":["### Tokenize again"]},{"cell_type":"code","execution_count":8,"id":"4f277b41","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4f277b41","outputId":"3a6d214b-f5e9-4d4f-b41d-6d58514bb0d3","executionInfo":{"status":"ok","timestamp":1719835708308,"user_tz":-120,"elapsed":13,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The president of the United States (US) is Joe Biden.\n","['The', 'president', 'of', 'the', 'United', 'States', '(US)', 'is', 'Joe', 'Biden.']\n","Joe Biden the president wants us in a united country.\n","['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country.']\n","Did a known artist paint portraits of Joe Biden?\n","['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden?']\n","A really well-known portrait artist is Vincent van Gogh.\n","['A', 'really', 'well-known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh.']\n","The leaves were left untouched, and the bats were unable to bat away the flies.\n","['The', 'leaves', 'were', 'left', 'untouched,', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies.']\n"]}],"source":["for sentence in sentences:\n","    print(sentence)\n","    words = sentence.split()\n","    print(words)"]},{"cell_type":"markdown","id":"8fc1c209","metadata":{"id":"8fc1c209"},"source":["### Unify 1 - getting rid of non-words\n","\n","\n","This regular expression below (re.split(r'\\W+', sentence)) splits the input sentence into words by using any sequence of non-word characters (such as spaces, punctuation, etc.) as delimiters. This is useful for preprocessing text because it effectively separates and extracts individual words, making the text easier to process, analyze, or manipulate for tasks like word frequency analysis, natural language processing, or text mining."]},{"cell_type":"code","execution_count":9,"id":"a91aff2f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a91aff2f","outputId":"5b438787-0953-487a-b1f2-f447ca25f5b5","executionInfo":{"status":"ok","timestamp":1719835708308,"user_tz":-120,"elapsed":10,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The president of the United States (US) is Joe Biden.\n","['The', 'president', 'of', 'the', 'United', 'States', 'US', 'is', 'Joe', 'Biden', '']\n","Joe Biden the president wants us in a united country.\n","['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country', '']\n","Did a known artist paint portraits of Joe Biden?\n","['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden', '']\n","A really well-known portrait artist is Vincent van Gogh.\n","['A', 'really', 'well', 'known', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh', '']\n","The leaves were left untouched, and the bats were unable to bat away the flies.\n","['The', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies', '']\n"]}],"source":["for sentence in sentences:\n","    print(sentence)\n","    words = re.split(r'\\W+', sentence)\n","    print(words)"]},{"cell_type":"markdown","id":"d20be0ec","metadata":{"id":"d20be0ec"},"source":["### Unify 2 - more elegant\n","\n","In the context below of re.sub(r'\\W+', '', word), the term \"sub\" stands for \"substitute\". It's a function in Python's regular expression (re) module that is used for substituting all occurrences of a specified pattern in a given string with another string. Here, the pattern \\W+ (which matches any sequence of non-word characters) is being replaced with an empty string '', effectively removing these characters from the input word. This substitution feature is a powerful tool in text processing for modifying and cleaning data.\n","\n","Why the part \"if strip(w)\" in the command [strip(w) for w in words if strip(w)]? Because it only allows elements in that are not empty. This can happen if several non-word characters follow each other."]},{"cell_type":"code","execution_count":10,"id":"b2dc0934","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2dc0934","outputId":"cb36c505-a30b-467b-c72a-5c17d07efeef","executionInfo":{"status":"ok","timestamp":1719835708308,"user_tz":-120,"elapsed":8,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'president', 'of', 'the', 'United', 'States', 'US', 'is', 'Joe', 'Biden']\n","['Joe', 'Biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country']\n","['Did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'Joe', 'Biden']\n","['A', 'really', 'wellknown', 'portrait', 'artist', 'is', 'Vincent', 'van', 'Gogh']\n","['The', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies']\n"]}],"source":["#let's write our own function to do this\n","def strip(word):\n","    mod_string = re.sub(r'\\W+', '', word)\n","    return mod_string\n","\n","for sentence in sentences:\n","    words = sentence.split()\n","    stripped = [strip(w) for w in words if strip(w)]\n","    print(stripped)"]},{"cell_type":"markdown","id":"3206078c","metadata":{"id":"3206078c"},"source":["### Unify 3 - add lowercasing\n","\n","Lowercasing is super easy to integrate in this pipeline."]},{"cell_type":"code","execution_count":11,"id":"ca2a457b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca2a457b","outputId":"b84e580a-a9bc-4fd4-a19d-aab7e12db082","executionInfo":{"status":"ok","timestamp":1719835708308,"user_tz":-120,"elapsed":6,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'president', 'of', 'the', 'united', 'states', 'us', 'is', 'joe', 'biden']\n","['joe', 'biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country']\n","['did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'joe', 'biden']\n","['a', 'really', 'wellknown', 'portrait', 'artist', 'is', 'vincent', 'van', 'gogh']\n","['the', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies']\n"]}],"source":["for sentence in sentences:\n","    words = sentence.split()\n","    lowered = [strip(w).lower() for w in words]\n","    print(lowered)"]},{"cell_type":"markdown","id":"650a74eb","metadata":{"id":"650a74eb"},"source":["### Do you see a problem?\n","\n","There is a problem with the code above. Depending on your application you don't like what it does.\n","\n","What does the code below do? (remove period to check)\n","\n","### Unify 4 - special lowercasing"]},{"cell_type":"code","execution_count":12,"id":"ca1da24b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca1da24b","outputId":"f7914ea4-b896-457c-e1e2-c0b68cab87c9","executionInfo":{"status":"ok","timestamp":1719835708308,"user_tz":-120,"elapsed":5,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'president', 'of', 'the', 'united', 'states', 'US', 'is', 'joe', 'biden']\n","['joe', 'biden', 'the', 'president', 'wants', 'us', 'in', 'a', 'united', 'country']\n","['did', 'a', 'known', 'artist', 'paint', 'portraits', 'of', 'joe', 'biden']\n","['a', 'really', 'wellknown', 'portrait', 'artist', 'is', 'vincent', 'van', 'gogh']\n","['the', 'leaves', 'were', 'left', 'untouched', 'and', 'the', 'bats', 'were', 'unable', 'to', 'bat', 'away', 'the', 'flies']\n"]}],"source":["def abbr_or_lower(word):\n","    if re.match('([A-Z]+[a-z]*){2,}', word):\n","        return word\n","    else:\n","        return word.lower()\n","\n","corpus=[]\n","for sentence in sentences:\n","    words = sentence.split()\n","    lowered = [abbr_or_lower(strip(w)) for w in words]\n","    print(lowered)\n","    corpus.append(lowered)"]},{"cell_type":"markdown","id":"a73e2701","metadata":{"id":"a73e2701"},"source":["### Apply the unification after Lemmatization\n","\n","We return to the POS plus lemmatization pipeline above and now add the new tricks after this."]},{"cell_type":"code","execution_count":13,"id":"7ae46518","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ae46518","outputId":"5f99e5ba-63b9-4c15-c660-abf94a2f1869","executionInfo":{"status":"ok","timestamp":1719835708639,"user_tz":-120,"elapsed":335,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'president', 'of', 'the', 'united', 'states', 'US', 'be', 'joe', 'biden']\n","['joe', 'biden', 'the', 'president', 'want', 'we', 'in', 'a', 'united', 'country']\n","['do', 'a', 'known', 'artist', 'paint', 'portrait', 'of', 'joe', 'biden']\n","['a', 'really', 'well', 'know', 'portrait', 'artist', 'be', 'vincent', 'van', 'gogh']\n","['the', 'leave', 'be', 'leave', 'untouched', 'and', 'the', 'bat', 'be', 'unable', 'to', 'bat', 'away', 'the', 'fly']\n"]}],"source":["#we use the exact same loop but now collect in a list\n","corpus_pos_lemm=[]\n","for sentence in sentences:\n","    sp_text=sp(sentence)\n","    lemmatized_sentence = []\n","    for word in sp_text:\n","        lemmatized_sentence.append(word.lemma_)\n","    #if abbr_or_lower(w).strip() gets rid of empties ''\n","    lowered = [abbr_or_lower(strip(w)) for w in lemmatized_sentence if abbr_or_lower(strip(w))]\n","    print(lowered)\n","    corpus_pos_lemm.append(lowered)\n"]},{"cell_type":"markdown","id":"8dba5adb","metadata":{"id":"8dba5adb"},"source":["## Stopwords\n","\n","This is a very interesting concept. Stopwords are something we agree on are generally not meaningful to distinguish one document from another.\n","\n","Note: the idea of stopwords is closely linked to the bag-of-words model where grammar doesn't play a role."]},{"cell_type":"code","execution_count":14,"id":"14a0b68c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14a0b68c","outputId":"265e3322-ac82-4370-85de-bb5e48afa252","executionInfo":{"status":"ok","timestamp":1719835708639,"user_tz":-120,"elapsed":14,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['president', 'united', 'states', 'US', 'joe', 'biden']\n","['joe', 'biden', 'president', 'wants', 'us', 'united', 'country']\n","['known', 'artist', 'paint', 'portraits', 'joe', 'biden']\n","['really', 'wellknown', 'portrait', 'artist', 'vincent', 'van', 'gogh']\n","['leaves', 'left', 'untouched', 'bats', 'unable', 'bat', 'away', 'flies']\n"]}],"source":["corpus_stop=[]\n","for sentence in sentences:\n","    words = sentence.split()\n","    lowered = [abbr_or_lower(strip(w)) for w in words if abbr_or_lower(strip(w)) not in set(stopwords.words('english'))]\n","    print(lowered)\n","    corpus_stop.append(lowered)\n"]},{"cell_type":"markdown","id":"b7b088a1","metadata":{"id":"b7b088a1"},"source":["## Stemming\n","\n","\"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\"\n","\n","nltk allows you to choose between PorterStammer or LancasterStammer, PorterStemmer being the oldest one originally developed in 1979. LancasterStemmer was developed in 1990 and uses a more aggressive approach than Porter Stemming Algorithm. Then we have the SnowballStemmer.\n","\n","For Spanish we have the SnowballStemmer.\n","\n","Stemming is standard in text mining. But it does not always make sense. My experience is that it really depends on the application whether this is worth it. One important aspect is that sometimes stemming is quite brutal leading to very harsh cuts. Can be useful if dimensionality reduction is an important concern."]},{"cell_type":"code","execution_count":15,"id":"fc710d81","metadata":{"id":"fc710d81","executionInfo":{"status":"ok","timestamp":1719835708639,"user_tz":-120,"elapsed":12,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[],"source":["from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","#english and spanish language\n","from nltk.stem.snowball import SnowballStemmer\n","\n","porter = PorterStemmer()\n","lancaster=LancasterStemmer()\n","englishStemmer=SnowballStemmer(\"english\")\n","spanishStemmer=SnowballStemmer(\"spanish\")"]},{"cell_type":"code","execution_count":16,"id":"246e0c5e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"246e0c5e","outputId":"eab0fe49-6646-44cf-9908-cfaddea94db7","executionInfo":{"status":"ok","timestamp":1719835708639,"user_tz":-120,"elapsed":11,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Word                Porter Stemmer      Lancaster Stemmer   Snowball Stemmer              \n","friend              friend              friend              friend                        \n","friendship          friendship          friend              friendship                    \n","friends             friend              friend              friend                        \n","friendships         friendship          friend              friendship                    \n","stable              stabl               stabl               stabl                         \n","stabilize           stabil              stabl               stabil                        \n","stabilized          stabil              stabl               stabil                        \n","misunderstanding    misunderstand       misunderstand       misunderstand                 \n","footballers         footbal             footbal             footbal                       \n","fighter             fighter             fight               fighter                       \n","fought              fought              fought              fought                        \n"]}],"source":["#A list of words to be stemmed\n","word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stable\",\"stabilize\",\"stabilized\", \"misunderstanding\",\"footballers\", \"fighter\", \"fought\"]\n","print(\"{0:20}{1:20}{2:20}{3:30}\".format(\"Word\",\"Porter Stemmer\",\"Lancaster Stemmer\", \"Snowball Stemmer\"))\n","for word in word_list:\n","    print(\"{0:20}{1:20}{2:20}{3:30}\".format(word,porter.stem(word),lancaster.stem(word), englishStemmer.stem(word)))"]},{"cell_type":"code","execution_count":17,"id":"3ca7bf7f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ca7bf7f","outputId":"70cc17f7-932d-449a-ac7c-7e02d9150484","executionInfo":{"status":"ok","timestamp":1719835708640,"user_tz":-120,"elapsed":11,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Word                Snowball Stemmer    \n","amigo               amig                \n","amigos              amig                \n","amigas              amig                \n","correr              corr                \n","corriente           corrient            \n","incertidumbre       incertidumbr        \n","luchadores          luchador            \n","desempleo           desemple            \n","fueron              fueron              \n"]}],"source":["#A spanish list of words to be stemmed\n","word_list = [\"amigo\", \"amigos\", \"amigas\", \"correr\",\"corriente\",\"incertidumbre\",\"luchadores\", \"desempleo\",\"fueron\"]\n","print(\"{0:20}{1:20}\".format(\"Word\",\"Snowball Stemmer\"))\n","for word in word_list:\n","    print(\"{0:20}{1:20}\".format(word,spanishStemmer.stem(word)))"]},{"cell_type":"markdown","id":"43224711","metadata":{"id":"43224711"},"source":["#### Bringing stemming back to the pipeline\n","\n","Note that the stemmer has a lower case maker but not a punctuation strip included. I therefore kicked out the abbr_or_lower() function but left the strip(w) in place."]},{"cell_type":"code","execution_count":18,"id":"2f25ce3a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2f25ce3a","outputId":"2f7587ff-b4c9-4619-bd32-db0a7f93be24","executionInfo":{"status":"ok","timestamp":1719835708640,"user_tz":-120,"elapsed":9,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'presid', 'of', 'the', 'unit', 'state', 'us', 'is', 'joe', 'biden']\n","['joe', 'biden', 'the', 'presid', 'want', 'us', 'in', 'a', 'unit', 'countri']\n","['did', 'a', 'known', 'artist', 'paint', 'portrait', 'of', 'joe', 'biden']\n","['a', 'realli', 'wellknown', 'portrait', 'artist', 'is', 'vincent', 'van', 'gogh']\n","['the', 'leav', 'were', 'left', 'untouch', 'and', 'the', 'bat', 'were', 'unabl', 'to', 'bat', 'away', 'the', 'fli']\n"]}],"source":["corpus_stem=[]\n","for sentence in sentences:\n","    words = sentence.split()\n","    lowered = [englishStemmer.stem(strip(w)) for w in words]\n","    print(lowered)\n","    corpus_stem.append(lowered)"]},{"cell_type":"markdown","id":"7109c36f","metadata":{"id":"7109c36f"},"source":["## Vectorization\n","\n","Soon comes the most important step in the pipeline. We will now represent the sentences as vectors - through a count of their terms. We will for now do this is the simplest possible form without dropping anything and focusing on unigrams.\n","\n","Conceptually important here is the fact that we want to construct a dictionary of how positions in the vector (the index) relate to terms. But we can then forget about the meaning of the index and just treat the documents vectors of numbers. This is wonderful as it allows us to do all the cool stuff that we can do with vectors.\n","\n","The following code goes through the corpus that we made above. Let's try different ways of making the corpus and think about the vectors this generates.\n","\n","### Start with lowercased corpus to build vocabulary"]},{"cell_type":"code","execution_count":19,"id":"54aaa8f7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54aaa8f7","outputId":"8d76d0c7-b238-4afd-d288-30078e8a32d7","executionInfo":{"status":"ok","timestamp":1719835708913,"user_tz":-120,"elapsed":280,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 1, 'president': 2, 'of': 3, 'united': 4, 'states': 5, 'US': 6, 'is': 7, 'joe': 8, 'biden': 9, 'wants': 10, 'us': 11, 'in': 12, 'a': 13, 'country': 14, 'did': 15, 'known': 16, 'artist': 17, 'paint': 18, 'portraits': 19, 'really': 20, 'wellknown': 21, 'portrait': 22, 'vincent': 23, 'van': 24, 'gogh': 25, 'leaves': 26, 'were': 27, 'left': 28, 'untouched': 29, 'and': 30, 'bats': 31, 'unable': 32, 'to': 33, 'bat': 34, 'away': 35, 'flies': 36}\n"," \n","Total size of vocabulary is: 36\n"]}],"source":["vocab, index = {}, 1  # start indexing from 1\n","for doc in corpus:\n","    for token in doc:\n","      if token not in vocab:\n","        vocab[token] = index\n","        index += 1\n","\n","vocab_lower=vocab\n","vocab_lower_size = len(vocab)\n","print(vocab_lower)\n","print(\" \")\n","print(\"Total size of vocabulary is:\", vocab_lower_size)"]},{"cell_type":"markdown","id":"acb8b24c","metadata":{"id":"acb8b24c"},"source":["### Stopword removed corpus\n","\n","Check out the total size of the vocabulary below. Why do we lose dimensions compared to the lowercased version?"]},{"cell_type":"code","execution_count":20,"id":"6fc5fcdd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fc5fcdd","outputId":"f2465894-bd8e-4257-e242-b180a47920d8","executionInfo":{"status":"ok","timestamp":1719835708914,"user_tz":-120,"elapsed":15,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'president': 1, 'united': 2, 'states': 3, 'US': 4, 'joe': 5, 'biden': 6, 'wants': 7, 'us': 8, 'country': 9, 'known': 10, 'artist': 11, 'paint': 12, 'portraits': 13, 'really': 14, 'wellknown': 15, 'portrait': 16, 'vincent': 17, 'van': 18, 'gogh': 19, 'leaves': 20, 'left': 21, 'untouched': 22, 'bats': 23, 'unable': 24, 'bat': 25, 'away': 26, 'flies': 27}\n"," \n","Total size of vocabulary with stopwords removed is: 27\n"]}],"source":["vocab, index = {}, 1  # start indexing from 1\n","for doc in corpus_stop:\n","    for token in doc:\n","      if token not in vocab:\n","        vocab[token] = index\n","        index += 1\n","\n","vocab_stop=vocab\n","vocab_stop_size = len(vocab)\n","print(vocab_stop)\n","print(\" \")\n","print(\"Total size of vocabulary with stopwords removed is:\", vocab_stop_size)"]},{"cell_type":"markdown","id":"72bda440","metadata":{"id":"72bda440"},"source":["### Stemmed corpus\n","\n","Why do we lose dimensions compared to the lowercased version?"]},{"cell_type":"code","execution_count":21,"id":"281ee802","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"281ee802","outputId":"bae392d5-dec8-480e-cc88-85185ad5d8a1","executionInfo":{"status":"ok","timestamp":1719835708914,"user_tz":-120,"elapsed":14,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 1, 'presid': 2, 'of': 3, 'unit': 4, 'state': 5, 'us': 6, 'is': 7, 'joe': 8, 'biden': 9, 'want': 10, 'in': 11, 'a': 12, 'countri': 13, 'did': 14, 'known': 15, 'artist': 16, 'paint': 17, 'portrait': 18, 'realli': 19, 'wellknown': 20, 'vincent': 21, 'van': 22, 'gogh': 23, 'leav': 24, 'were': 25, 'left': 26, 'untouch': 27, 'and': 28, 'bat': 29, 'unabl': 30, 'to': 31, 'away': 32, 'fli': 33}\n"," \n","Total size of stemmed vocabulary: 33\n"]}],"source":["vocab, index = {}, 1  # start indexing from 1\n","for doc in corpus_stem:\n","    for token in doc:\n","      if token not in vocab:\n","        vocab[token] = index\n","        index += 1\n","\n","vocab_stem=vocab\n","vocab_stem_size = len(vocab)\n","print(vocab_stem)\n","print(\" \")\n","print(\"Total size of stemmed vocabulary:\", vocab_stem_size)"]},{"cell_type":"markdown","id":"36afacb6","metadata":{"id":"36afacb6"},"source":["### Lemmatize then unified\n","\n","Why do we lose dimensions compared to the lowercased version?"]},{"cell_type":"code","execution_count":22,"id":"b80fa22e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b80fa22e","outputId":"b35cba0c-29ef-4120-b914-55a1dad93228","executionInfo":{"status":"ok","timestamp":1719835708914,"user_tz":-120,"elapsed":12,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 1, 'president': 2, 'of': 3, 'united': 4, 'states': 5, 'US': 6, 'be': 7, 'joe': 8, 'biden': 9, 'want': 10, 'we': 11, 'in': 12, 'a': 13, 'country': 14, 'do': 15, 'known': 16, 'artist': 17, 'paint': 18, 'portrait': 19, 'really': 20, 'well': 21, 'know': 22, 'vincent': 23, 'van': 24, 'gogh': 25, 'leave': 26, 'untouched': 27, 'and': 28, 'bat': 29, 'unable': 30, 'to': 31, 'away': 32, 'fly': 33}\n"," \n","Total size of stemmed vocabulary: 33\n"]}],"source":["vocab, index = {}, 1  # start indexing from 1\n","for doc in corpus_pos_lemm:\n","    for token in doc:\n","      if token not in vocab:\n","        vocab[token] = index\n","        index += 1\n","\n","vocab_pos_lemm=vocab\n","vocab_pos_lemm_size = len(vocab)\n","print(vocab_pos_lemm)\n","print(\" \")\n","print(\"Total size of stemmed vocabulary:\", vocab_pos_lemm_size)"]},{"cell_type":"markdown","id":"6eb13c67","metadata":{"id":"6eb13c67"},"source":["### Moving on. Let's vectorize our documents using the lowercased vocabulary.\n","\n","Let's run a count of terms in the vocab_lower on the first lowercased sentense.\n","\n","Make sure you understand what the following cell does."]},{"cell_type":"code","execution_count":23,"id":"70d19355","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70d19355","outputId":"33b4961b-ec95-4c42-f217-4f5ff803b193","executionInfo":{"status":"ok","timestamp":1719835708914,"user_tz":-120,"elapsed":11,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["the 3\n","president 0\n","of 0\n","united 0\n","states 0\n","US 0\n","is 0\n","joe 0\n","biden 0\n","wants 0\n","us 0\n","in 0\n","a 0\n","country 0\n","did 0\n","known 0\n","artist 0\n","paint 0\n","portraits 0\n","really 0\n","wellknown 0\n","portrait 0\n","vincent 0\n","van 0\n","gogh 0\n","leaves 1\n","were 2\n","left 1\n","untouched 1\n","and 1\n","bats 1\n","unable 1\n","to 1\n","bat 1\n","away 1\n","flies 1\n"]}],"source":["for w in vocab_lower:\n","    print(w, corpus[4].count(w))"]},{"cell_type":"markdown","id":"968eadd2","metadata":{"id":"968eadd2"},"source":["We will write a function that uses this. Make sure you understand what the following thing does."]},{"cell_type":"code","execution_count":24,"id":"21d80b0c","metadata":{"id":"21d80b0c","executionInfo":{"status":"ok","timestamp":1719835708914,"user_tz":-120,"elapsed":10,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[],"source":["def vectorize(tokens, vocab):\n","    vector=[]\n","    for w in vocab:\n","        vector.append(tokens.count(w))\n","    return vector"]},{"cell_type":"markdown","id":"06ed8ab2","metadata":{"id":"06ed8ab2"},"source":["Now we apply this to the entire lowercased corpus, document by document."]},{"cell_type":"code","execution_count":25,"id":"150990d3","metadata":{"id":"150990d3","executionInfo":{"status":"ok","timestamp":1719835708914,"user_tz":-120,"elapsed":10,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[],"source":["vectors=[]\n","for doc in corpus:\n","    vectors.append(vectorize(doc, vocab_lower))\n"]},{"cell_type":"markdown","id":"1907a256","metadata":{"id":"1907a256"},"source":["Let's visualize this collection of vectors that we have generated from the documents making counts of terms in the dictionary."]},{"cell_type":"code","execution_count":26,"id":"940e3642","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"940e3642","outputId":"eac1e967-f82d-4746-a39b-4c504f5bc4b3","executionInfo":{"status":"ok","timestamp":1719835708914,"user_tz":-120,"elapsed":9,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   0   1   2   3   4   5   6   7   8   9   ...  26  27  28  29  30  31  32  \\\n","0   2   1   1   1   1   1   1   1   1   0  ...   0   0   0   0   0   0   0   \n","1   1   1   0   1   0   0   0   1   1   1  ...   0   0   0   0   0   0   0   \n","2   0   0   1   0   0   0   0   1   1   0  ...   0   0   0   0   0   0   0   \n","3   0   0   0   0   0   0   1   0   0   0  ...   0   0   0   0   0   0   0   \n","4   3   0   0   0   0   0   0   0   0   0  ...   2   1   1   1   1   1   1   \n","\n","   33  34  35  \n","0   0   0   0  \n","1   0   0   0  \n","2   0   0   0  \n","3   0   0   0  \n","4   1   1   1  \n","\n","[5 rows x 36 columns]"],"text/html":["\n","  <div id=\"df-7bcb02c9-4a5d-41df-a264-8c805e07d8a1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 36 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bcb02c9-4a5d-41df-a264-8c805e07d8a1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7bcb02c9-4a5d-41df-a264-8c805e07d8a1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7bcb02c9-4a5d-41df-a264-8c805e07d8a1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5b704cf6-3bb6-4733-ad1b-9fd6b864b2d4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b704cf6-3bb6-4733-ad1b-9fd6b864b2d4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5b704cf6-3bb6-4733-ad1b-9fd6b864b2d4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_715008d5-610d-4bb7-9b84-62c78363209f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_715008d5-610d-4bb7-9b84-62c78363209f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":26}],"source":["#making a pandas dataframe\n","df = pd.DataFrame(vectors)\n","df"]},{"cell_type":"markdown","id":"94a98ce1","metadata":{"id":"94a98ce1"},"source":["The above is a huge step. This is a mathematical vector representation of the corpus! This is how machines in the bag-of-words model see language.\n","\n","## How is this animal called?\n","\n","The name of this matrix has been mentioned above and in the lecture.\n","\n","This is a key item we will come back to again and again - it is called the..."]},{"cell_type":"markdown","id":"10f3214f","metadata":{"id":"10f3214f"},"source":["### What are the correct column names here?\n","For the machine the columns of the document-term matrix are index numbers. But for humans it is useful to think about what the columns represent. The vocab_lower dictionaries has them stored. What will they be?\n","\n","Think before removing the period."]},{"cell_type":"code","execution_count":27,"id":"7c8ef6e1","metadata":{"id":"7c8ef6e1","outputId":"69084cfb-bfb7-4a45-d074-24bebd64b882","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"error","timestamp":1719835708915,"user_tz":-120,"elapsed":9,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-27-db6bbb118ee0>, line 3)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-db6bbb118ee0>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    .\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["df.columns = vocab_lower.keys()\n","df\n","."]},{"cell_type":"markdown","id":"b426cb74","metadata":{"id":"b426cb74"},"source":["### Distances between texts\n","\n","Let's have some short fun with the matrix. We will transpos the document term matrix and then look at the column vectors of this new matrix.\n","\n","Note that each column now is a representation of a document. As they are vectors we can now calculate the distances between the different sentences as shown in the lecture.\n","\n","\n","Remember the sentences were:\n","\n","sentence1 = \"The president of the United States (US) is Joe Biden.\"\n","\n","sentence2 = \"Joe Biden the president wants us in a united country.\"\n","\n","sentence3 = \"Did a known artist paint portraits of Joe Biden?\"\n","\n","sentence4 = \"A really well-known portrait artist is Vincent van Gogh.\"\n","\n","sentence5 = \"The leaves were left untouched, and the bats were unable to bat away the flies.\"\n","\n","Which sentences are most similar do you think?\n"]},{"cell_type":"code","execution_count":28,"id":"a40b9365","metadata":{"id":"a40b9365","outputId":"223f7346-711f-4e43-8247-1bce2e2f6baf","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1719835802606,"user_tz":-120,"elapsed":245,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    0  1  2  3  4\n","0   2  1  0  0  3\n","1   1  1  0  0  0\n","2   1  0  1  0  0\n","3   1  1  0  0  0\n","4   1  0  0  0  0\n","5   1  0  0  0  0\n","6   1  0  0  1  0\n","7   1  1  1  0  0\n","8   1  1  1  0  0\n","9   0  1  0  0  0\n","10  0  1  0  0  0\n","11  0  1  0  0  0\n","12  0  1  1  1  0\n","13  0  1  0  0  0\n","14  0  0  1  0  0\n","15  0  0  1  0  0\n","16  0  0  1  1  0\n","17  0  0  1  0  0\n","18  0  0  1  0  0\n","19  0  0  0  1  0\n","20  0  0  0  1  0\n","21  0  0  0  1  0\n","22  0  0  0  1  0\n","23  0  0  0  1  0\n","24  0  0  0  1  0\n","25  0  0  0  0  1\n","26  0  0  0  0  2\n","27  0  0  0  0  1\n","28  0  0  0  0  1\n","29  0  0  0  0  1\n","30  0  0  0  0  1\n","31  0  0  0  0  1\n","32  0  0  0  0  1\n","33  0  0  0  0  1\n","34  0  0  0  0  1\n","35  0  0  0  0  1"],"text/html":["\n","  <div id=\"df-416fd5e5-5273-443d-a3a3-1209de0b7410\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-416fd5e5-5273-443d-a3a3-1209de0b7410')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-416fd5e5-5273-443d-a3a3-1209de0b7410 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-416fd5e5-5273-443d-a3a3-1209de0b7410');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-93be1742-6125-4c3c-ade2-19b85a0a2d9a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93be1742-6125-4c3c-ade2-19b85a0a2d9a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-93be1742-6125-4c3c-ade2-19b85a0a2d9a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_b102e264-d4e0-4505-8ab9-17cd37fee8a3\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b102e264-d4e0-4505-8ab9-17cd37fee8a3 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":28}],"source":["df=df.transpose()\n","df\n"]},{"cell_type":"code","execution_count":29,"id":"3e421d6b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e421d6b","outputId":"29acb255-492a-4346-b720-8a319121930c","executionInfo":{"status":"ok","timestamp":1719835802857,"user_tz":-120,"elapsed":4,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The president of the United States (US) is Joe Biden.\n","Joe Biden the president wants us in a united country.\n","Did a known artist paint portraits of Joe Biden?\n","A really well-known portrait artist is Vincent van Gogh.\n","The leaves were left untouched, and the bats were unable to bat away the flies.\n"," \n","Difference of sentence 1 to sentence 2 is 10\n","Difference of sentence 2 to sentence 2 is 0\n","Difference of sentence 3 to sentence 2 is 13\n","Difference of sentence 4 to sentence 2 is 17\n","Difference of sentence 5 to sentence 2 is 23\n"]}],"source":["#distance of vectors to the second sentence df[1]\n","\n","for sentence in sentences:\n","    print(sentence)\n","print(\" \")\n","for i in range(0,5):\n","    result=abs(df[1]-df[i])\n","    print(\"Difference of sentence\",i+1, \"to sentence 2 is\", result.sum(axis=0))\n","\n"]},{"cell_type":"markdown","id":"3f773388","metadata":{"id":"3f773388"},"source":["# WOW THIS IS AMAZING - YOUR LIFE HAS CHANGED\n","(a tiny little bit)"]},{"cell_type":"markdown","id":"2e866636","metadata":{"id":"2e866636"},"source":["# Exercise (difficult)\n","\n","Write yourself a pipeline for processing a csv file into a document term matrix using **lowercasing, stopword removal and stemming**. Write the pipeline such that you can apply the loop \"for sentence in sentences:\" above to the column of a pandas data frame. Use the pieces of the pipeline we discussed in class. Do not use other packages."]},{"cell_type":"code","execution_count":29,"id":"7b6d71c3","metadata":{"id":"7b6d71c3","executionInfo":{"status":"ok","timestamp":1719835805255,"user_tz":-120,"elapsed":5,"user":{"displayName":"Miguel Conner","userId":"08317206551838505815"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1ACe_2W3NO9iQa4N2YGWXvPl4QlpInTk2","timestamp":1719590557139}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}